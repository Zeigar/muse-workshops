{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf15a5861d6d607fd230c107fb2248458500fb50"
   },
   "source": [
    "**<center><font size=5>EEG Data Analysis</font></center>**\n",
    "***<center>Alcoholic vs Control Groups</center>***\n",
    "***\n",
    "**author**: Ruslan Klymentiev\n",
    "\n",
    "**date**: 5th January, 2019\n",
    "\n",
    "**[GitHub Repository](https://github.com/ruslan-kl/EEG-data-analysis)**\n",
    "\n",
    "#### Table of Contents\n",
    "- <a href='#intro'>1. Project Overview</a> \n",
    "- <a href='#env'>2. Setting up the Environment</a>\n",
    " - <a href='#import'>2.1. Data Import</a>\n",
    " - <a href='#var_des'>2.2. Variables description</a>\n",
    "- <a href='#sample'>3. Data Sample Visualization</a> \n",
    " - <a href='#s1'>3.1. Data Sample for \"S1 obj\" Stimulus</a>\n",
    " - <a href='#s2m'>3.2. Data Sample for \"S2 match\" Stimulus</a>\n",
    " - <a href='#s2nm'>3.3. Data Sample for \"S2 nomatch\" Stimulus</a>\n",
    " - <a href='#corr_sample'>3.4. Correlations between the Regions</a>\n",
    "- <a href='#whole_set'>4. Whole EEG Data Set Analysis</a> \n",
    " - <a href='#corr'>4.1. Correlations between the Regions</a>\n",
    " - <a href='#avg_comp'>4.2. Compare the Average Response Values</a>\n",
    "- <a href='#conclusion'>5. Conclusions</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7817e10dc2ddd8b5c6e3e7426112140d22258579"
   },
   "source": [
    "# <a id='intro'>1. Project Overview</a>\n",
    "\n",
    "It's been a while since a posted something on Kaggle, so I thought that it is time to change it. I have chosen the [EEG-Alcohol](https://www.kaggle.com/nnair25/Alcoholics) dataset, which contains [EEG (Electroencephalography)](https://en.wikipedia.org/wiki/Electroencephalography) data for two groups - Alcoholic and Control Group. \n",
    "![](https://i.imgur.com/ZrmxJRu.jpg)\n",
    "Amount of subjects in each group is 8. The 64 electrodes were placed on subject's scalps to measure theelectrical activity of the brain. The response values were sampled at 256 Hz (3.9-msec epoch) for 1 second. Each subject was exposed to either a single stimulus (S1) or to two stimuli (S1 and S2) which were pictures of objects chosen from the [1980 Snodgrass and Vanderwart picture set](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.1979&rep=rep1&type=pdf). When two stimuli were shown, they were presented in either a matched condition where S1 was identical to S2 or in a non-matched condition where S1 differed from S2.\n",
    "\n",
    "The purpose of my analysis is going to be to find out if there is a difference in response values for different stimuli between control and alcoholic group. And if so, in what brain regions this happens? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aaaf7f239981ae7487ccfb178472eb3ba5ccb76c"
   },
   "source": [
    "# <a id='env'>2. Setting up the Environment</a>\n",
    "## <a id='import'>2.1. Data Import</a>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/05/3c32c6bc85acbd30a18fbc3ba732fed5e48e5f8fd60d2a148877970f4a61/plotly-4.2.1-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2MB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting retrying>=1.3.3 (from plotly)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
      "Requirement already satisfied: six in /Users/SaarangP/anaconda3/lib/python3.7/site-packages (from plotly) (1.12.0)\n",
      "Building wheels for collected packages: retrying\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/SaarangP/Library/Caches/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
      "Successfully built retrying\n",
      "Installing collected packages: retrying, plotly\n",
      "Successfully installed plotly-4.2.1 retrying-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "execution_count": 11,
>>>>>>> e0b056df339aa3119da9d4d4a1edc5ac9e78d688
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "init_notebook_mode(connected=True) ## plotly init\n",
    "seed = 123\n",
    "random.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the data from [EEG-Alcohol](https://www.kaggle.com/nnair25/Alcoholics) and put it into this directory"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 16,
>>>>>>> e0b056df339aa3119da9d4d4a1edc5ac9e78d688
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6d213deb9d06151db70b9975bcc2bb6fa78bffc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of files in SMNI_CMI_TRAIN directory: 469\n"
     ]
    }
   ],
   "source": [
    "print('Total amount of files in SMNI_CMI_TRAIN directory: ' + str(len(os.listdir('../Workshop 2/SMNI_CMI_TRAIN/'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "78519635c2217182f4c8e13bc25876631eb8f8a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      " 35%|███▍      | 163/469 [02:28<17:19,  3.40s/it]"
=======
      "\n",
      "\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1/469 [00:00<01:19,  5.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/469 [00:00<01:10,  6.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 3/469 [00:00<01:36,  4.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 4/469 [00:00<01:21,  5.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 5/469 [00:00<01:12,  6.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 6/469 [00:00<01:06,  6.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 7/469 [00:01<01:01,  7.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 8/469 [00:01<01:07,  6.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 9/469 [00:01<01:03,  7.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 10/469 [00:01<01:00,  7.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 11/469 [00:01<00:59,  7.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 12/469 [00:01<00:59,  7.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 13/469 [00:01<00:58,  7.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 14/469 [00:02<01:05,  6.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 15/469 [00:02<01:04,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 16/469 [00:02<01:04,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 17/469 [00:02<01:04,  7.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 18/469 [00:02<01:05,  6.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 19/469 [00:02<01:06,  6.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 20/469 [00:02<01:13,  6.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 21/469 [00:03<01:13,  6.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 22/469 [00:03<01:12,  6.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 23/469 [00:03<01:12,  6.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 24/469 [00:03<01:13,  6.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 25/469 [00:03<01:20,  5.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 26/469 [00:04<01:32,  4.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 27/469 [00:04<01:31,  4.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 28/469 [00:04<01:32,  4.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 29/469 [00:04<01:33,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 30/469 [00:04<01:33,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 31/469 [00:05<01:32,  4.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 32/469 [00:05<01:36,  4.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 33/469 [00:05<01:33,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 34/469 [00:05<01:33,  4.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 35/469 [00:06<01:33,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 36/469 [00:06<01:33,  4.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 37/469 [00:06<01:40,  4.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 38/469 [00:06<01:37,  4.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 39/469 [00:06<01:37,  4.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 40/469 [00:07<01:36,  4.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 41/469 [00:07<01:37,  4.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 42/469 [00:07<01:38,  4.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 43/469 [00:07<01:44,  4.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 44/469 [00:08<01:42,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 45/469 [00:08<01:43,  4.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 46/469 [00:08<01:42,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 47/469 [00:08<01:43,  4.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 48/469 [00:09<01:43,  4.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 49/469 [00:09<01:50,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 50/469 [00:09<01:50,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 51/469 [00:09<01:49,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 52/469 [00:10<01:49,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 53/469 [00:10<01:51,  3.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 54/469 [00:10<01:53,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 55/469 [00:11<02:04,  3.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 56/469 [00:11<02:01,  3.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 57/469 [00:11<01:59,  3.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 58/469 [00:12<02:02,  3.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 59/469 [00:12<02:05,  3.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 60/469 [00:12<02:13,  3.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 61/469 [00:13<02:15,  3.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 62/469 [00:13<02:17,  2.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 63/469 [00:13<02:19,  2.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 64/469 [00:14<02:16,  2.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 65/469 [00:14<02:14,  3.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 66/469 [00:14<02:18,  2.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 67/469 [00:15<02:16,  2.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 68/469 [00:15<02:19,  2.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 69/469 [00:15<02:23,  2.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 70/469 [00:16<02:29,  2.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 71/469 [00:16<02:31,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 72/469 [00:17<02:37,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 73/469 [00:17<02:29,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 74/469 [00:17<02:33,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 75/469 [00:18<02:36,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 76/469 [00:18<02:39,  2.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 77/469 [00:19<02:35,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 78/469 [00:19<02:42,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 79/469 [00:19<02:36,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 80/469 [00:20<02:32,  2.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 81/469 [00:20<02:29,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 82/469 [00:21<02:29,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 83/469 [00:21<02:27,  2.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 84/469 [00:21<02:34,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 85/469 [00:22<02:32,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 86/469 [00:22<02:31,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 87/469 [00:23<02:29,  2.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 88/469 [00:23<02:29,  2.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 89/469 [00:23<02:35,  2.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 90/469 [00:24<02:31,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 91/469 [00:24<02:30,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 92/469 [00:25<02:30,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 93/469 [00:25<02:30,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 94/469 [00:25<02:31,  2.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 95/469 [00:26<02:37,  2.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 96/469 [00:26<02:36,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 97/469 [00:27<02:35,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 98/469 [00:27<02:37,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 99/469 [00:28<02:36,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 100/469 [00:28<02:37,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 101/469 [00:28<02:42,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 102/469 [00:29<02:41,  2.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 103/469 [00:29<02:43,  2.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 104/469 [00:30<02:46,  2.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 105/469 [00:30<02:52,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 106/469 [00:31<02:58,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 107/469 [00:31<03:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 108/469 [00:32<03:05,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 109/469 [00:33<03:10,  1.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 110/469 [00:33<03:05,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 111/469 [00:33<03:01,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 112/469 [00:34<02:57,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 113/469 [00:35<03:03,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 114/469 [00:35<03:04,  1.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 115/469 [00:36<03:06,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 116/469 [00:36<03:10,  1.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 117/469 [00:37<03:06,  1.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 118/469 [00:37<03:17,  1.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 119/469 [00:38<03:11,  1.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 120/469 [00:38<03:08,  1.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 121/469 [00:39<03:07,  1.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 122/469 [00:39<03:05,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 123/469 [00:40<03:05,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 124/469 [00:41<03:08,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 125/469 [00:41<03:08,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 126/469 [00:42<03:09,  1.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 127/469 [00:42<03:10,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 128/469 [00:43<03:08,  1.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 129/469 [00:43<03:07,  1.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 130/469 [00:44<03:30,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 131/469 [00:45<03:28,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 132/469 [00:45<03:26,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 133/469 [00:46<03:24,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 134/469 [00:46<03:21,  1.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 135/469 [00:47<03:21,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 136/469 [00:48<04:12,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 137/469 [00:49<03:58,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 138/469 [00:49<03:54,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 139/469 [00:50<03:57,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 140/469 [00:51<04:03,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 141/469 [00:52<03:57,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 142/469 [00:52<03:49,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 143/469 [00:53<03:43,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 144/469 [00:54<03:39,  1.48it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 145/469 [00:54<03:38,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 146/469 [00:55<03:39,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 147/469 [00:56<03:51,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 148/469 [00:57<04:00,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 149/469 [00:57<04:03,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 150/469 [00:58<04:16,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 151/469 [00:59<04:10,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 152/469 [01:00<03:56,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 153/469 [01:01<04:02,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 154/469 [01:01<04:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 155/469 [01:02<04:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 156/469 [01:03<03:51,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 157/469 [01:03<03:46,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 158/469 [01:04<03:43,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 159/469 [01:05<03:54,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 160/469 [01:06<03:44,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 161/469 [01:06<03:37,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 162/469 [01:07<03:33,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 163/469 [01:08<03:30,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 164/469 [01:08<03:29,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 165/469 [01:09<03:33,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 166/469 [01:10<03:33,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 167/469 [01:11<03:32,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 168/469 [01:11<03:36,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 169/469 [01:12<03:45,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 170/469 [01:13<03:49,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 171/469 [01:14<03:50,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 172/469 [01:14<03:49,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 173/469 [01:15<03:42,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 174/469 [01:16<03:37,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 175/469 [01:17<03:36,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 176/469 [01:17<03:39,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 177/469 [01:18<03:37,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 178/469 [01:19<03:48,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 179/469 [01:20<03:53,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 180/469 [01:21<03:50,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 181/469 [01:21<03:50,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 182/469 [01:22<04:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 183/469 [01:23<04:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 184/469 [01:24<04:02,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 185/469 [01:25<03:55,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 186/469 [01:26<03:56,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 187/469 [01:27<03:52,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 188/469 [01:27<03:51,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 189/469 [01:28<03:46,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 190/469 [01:29<03:49,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 191/469 [01:30<03:47,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 192/469 [01:31<03:44,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 193/469 [01:31<03:46,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 194/469 [01:32<03:43,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 195/469 [01:33<03:43,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 196/469 [01:34<03:42,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 197/469 [01:35<03:40,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 198/469 [01:35<03:40,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 199/469 [01:36<03:48,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 200/469 [01:37<03:44,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 201/469 [01:38<03:40,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 202/469 [01:39<03:36,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 203/469 [01:40<03:34,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 204/469 [01:40<03:32,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 205/469 [01:41<03:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 206/469 [01:42<03:33,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 207/469 [01:43<03:48,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 208/469 [01:44<03:43,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 209/469 [01:45<03:38,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 210/469 [01:46<04:02,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 211/469 [01:47<04:03,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 212/469 [01:48<03:56,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 213/469 [01:48<03:50,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 214/469 [01:49<03:54,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 215/469 [01:50<04:00,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 216/469 [01:51<03:55,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 217/469 [01:52<04:03,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 218/469 [01:53<04:02,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 219/469 [01:54<04:05,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 220/469 [01:55<04:01,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 221/469 [01:56<03:56,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 222/469 [01:57<04:01,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 223/469 [01:58<04:00,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 224/469 [01:59<03:54,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 225/469 [02:00<03:48,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 226/469 [02:01<03:46,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 227/469 [02:02<03:48,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 228/469 [02:03<03:49,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 229/469 [02:04<03:46,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 230/469 [02:05<03:41,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 231/469 [02:06<03:39,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 232/469 [02:07<03:36,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 233/469 [02:07<03:34,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 234/469 [02:08<03:38,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 235/469 [02:09<03:35,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 236/469 [02:10<03:33,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 237/469 [02:11<03:32,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 238/469 [02:12<03:31,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 239/469 [02:13<03:31,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 240/469 [02:14<03:33,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 241/469 [02:15<03:31,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 242/469 [02:16<03:30,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 243/469 [02:17<03:31,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 244/469 [02:18<03:33,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 245/469 [02:19<03:35,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 246/469 [02:20<03:39,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 247/469 [02:21<03:46,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 248/469 [02:22<03:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 249/469 [02:23<03:54,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 250/469 [02:24<03:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 251/469 [02:25<03:53,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 252/469 [02:26<03:58,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 253/469 [02:27<03:55,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 254/469 [02:28<03:48,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 255/469 [02:29<03:43,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 256/469 [02:30<03:39,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 257/469 [02:31<03:40,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 258/469 [02:33<03:41,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 259/469 [02:34<03:42,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 260/469 [02:35<03:49,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 261/469 [02:36<03:45,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 262/469 [02:37<03:43,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 263/469 [02:38<03:43,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 264/469 [02:39<03:42,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 265/469 [02:40<03:44,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 266/469 [02:41<03:45,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 267/469 [02:42<03:41,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 268/469 [02:43<03:36,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 269/469 [02:45<03:43,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 270/469 [02:46<03:44,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 271/469 [02:47<03:41,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 272/469 [02:48<03:38,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 273/469 [02:49<03:34,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 274/469 [02:50<03:37,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 275/469 [02:51<03:34,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 276/469 [02:52<03:33,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 277/469 [02:54<03:37,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 278/469 [02:55<03:36,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 279/469 [02:56<03:36,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 280/469 [02:57<03:47,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 281/469 [02:58<03:44,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 282/469 [03:00<03:41,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 283/469 [03:01<03:36,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 284/469 [03:02<03:32,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 285/469 [03:03<03:33,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 286/469 [03:04<03:34,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 287/469 [03:05<03:32,  1.17s/it]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 288/469 [03:07<03:29,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 289/469 [03:08<03:28,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 290/469 [03:09<03:37,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 291/469 [03:10<03:40,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 292/469 [03:12<03:38,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 293/469 [03:13<03:35,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 294/469 [03:14<03:31,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 295/469 [03:15<03:41,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 296/469 [03:17<03:37,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 297/469 [03:18<03:33,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 298/469 [03:19<03:32,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 299/469 [03:20<03:27,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 300/469 [03:21<03:26,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 301/469 [03:23<03:24,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 302/469 [03:24<03:23,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 303/469 [03:25<03:24,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 304/469 [03:26<03:26,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 305/469 [03:28<03:21,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 306/469 [03:29<03:19,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 307/469 [03:30<03:16,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 308/469 [03:31<03:13,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 309/469 [03:32<03:14,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 310/469 [03:34<03:12,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 311/469 [03:35<03:10,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 312/469 [03:36<03:08,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 313/469 [03:37<03:07,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 314/469 [03:38<03:08,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 315/469 [03:40<03:11,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 316/469 [03:41<03:10,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 317/469 [03:42<03:08,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 318/469 [03:43<03:06,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 319/469 [03:45<03:05,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 320/469 [03:46<03:04,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 321/469 [03:47<03:05,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 322/469 [03:48<03:06,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 323/469 [03:50<03:06,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 324/469 [03:51<03:07,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 325/469 [03:52<03:08,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 326/469 [03:54<03:08,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 327/469 [03:55<03:05,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 328/469 [03:56<03:03,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 329/469 [03:58<03:01,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 330/469 [03:59<02:59,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 331/469 [04:00<02:59,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 332/469 [04:02<03:01,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 333/469 [04:03<03:01,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 334/469 [04:04<02:59,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 335/469 [04:06<03:05,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 336/469 [04:07<03:09,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 337/469 [04:09<03:14,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 338/469 [04:10<03:15,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 339/469 [04:12<03:10,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 340/469 [04:13<03:06,  1.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../Workshop 2/SMNI_CMI_TRAIN/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d9eebb12ca38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Workshop 2/SMNI_CMI_TRAIN/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## read from the file to df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mEEG_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEG_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## add the file data to the main df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    993\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m    994\u001b[0m                                      engine=engine))\n\u001b[0;32m--> 995\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[1;32m   1984\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[1;32m   1986\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../Workshop 2/SMNI_CMI_TRAIN/Train'"
>>>>>>> e0b056df339aa3119da9d4d4a1edc5ac9e78d688
     ]
    }
   ],
   "source": [
    "filenames_list = os.listdir('../Workshop 2/SMNI_CMI_TRAIN/') ## list of file names in the directory\n",
    "EEG_data = pd.DataFrame({}) ## create an empty df that will hold data from each file\n",
    "\n",
    "for file_name in tqdm(filenames_list):\n",
<<<<<<< HEAD
    "    temp_df = pd.read_csv('../Workshop 2/SMNI_CMI_TRAIN/' + file_name) ## read from the file to df\n",
=======
    "    temp_df = pd.read_csv('../Workshop 2/SMNI_CMI_TRAIN/' + file_name, engine='python') ## read from the file to df\n",
>>>>>>> e0b056df339aa3119da9d4d4a1edc5ac9e78d688
    "    EEG_data = EEG_data.append(temp_df) ## add the file data to the main df\n",
    "    \n",
    "EEG_data = EEG_data.drop(['Unnamed: 0'], axis=1) ## remove the unused column\n",
    "EEG_data.loc[EEG_data['matching condition'] == 'S2 nomatch,', 'matching condition'] =  'S2 nomatch' ## remove comma sign from stimulus name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9051e68389da1bc215d1b95aabf600f81756d7f"
   },
   "outputs": [],
   "source": [
    "## here is how the data looks like\n",
    "EEG_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f09f121d4cb3050436b39fcabc18feeaf9e12134"
   },
   "source": [
    "## <a id='var_des'>2.2. Variables description</a>\n",
    "\n",
    "* `trial number`: number of the trial (obviously) \n",
    "* `sensor postition`:  position of electrode placed on subject's scalp (based on International 10-20 system)\n",
    "<img src=\"https://i.ibb.co/M2GRQNz/670px-International-10-20-system-for-EEG-MCN-svg.png\" alt=\"670px-International-10-20-system-for-EEG-MCN-svg\" border=\"0\">\n",
    "* `sample num`: 0-255\n",
    "* `sensor value`: value in microvolts \n",
    "* `subject identifier`: `a` - Alcoholic; `c` - Control\n",
    "* `matching condition`: \n",
    " * `S1 obj` - a single object shown; \n",
    " * `S2 match` - object 2 shown in a matching condition (S1 was identical to S2), \n",
    " * `S2 nomatch` - object 2 shown in a non matching condition (S1 differed from S2)\n",
    "* `channel`: channel number (0-63). Basically, it's the same as `sensor position` column, so one of these columns can be dropped\n",
    "* `name`: a serial code assigned to each subject\n",
    "* `time`: inverse of `sample num` measured in seconds\n",
    "\n",
    "\n",
    "I have changed some `sensor position`s so they match the basics for head topography visualization. Also, I have removed those positions which equal to **X**, **Y** and **ND** since I couldnt figure out which regions they respond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5370eeaa4ad4108733f0d5d0b34b3a159b6aeee6"
   },
   "outputs": [],
   "source": [
    "## replace some 'sensor position' values\n",
    "EEG_data.loc[EEG_data['sensor position'] == 'AF1', 'sensor position'] = 'AF3'\n",
    "EEG_data.loc[EEG_data['sensor position'] == 'AF2', 'sensor position'] = 'AF4'\n",
    "EEG_data.loc[EEG_data['sensor position'] == 'PO1', 'sensor position'] = 'PO3'\n",
    "EEG_data.loc[EEG_data['sensor position'] == 'PO2', 'sensor position'] = 'PO4'\n",
    "## remove rows with undefined positions\n",
    "EEG_data = EEG_data[(EEG_data['sensor position'] != 'X') & (EEG_data['sensor position'] != 'Y') & (EEG_data['sensor position'] != 'nd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec88dea9b672fcc7e8d39f0c19c048af1c170b13"
   },
   "source": [
    "# <a id='sample'>3. Data Sample Visualization</a> \n",
    "\n",
    "In this section I will randomly choose a subject from each group and plot the 3-D surface and heatmaps of response values to visually inspect the difference among groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a65c53cff7b4204c62d28988c0453df894178535"
   },
   "outputs": [],
   "source": [
    "def sample_data(stimulus, random_id=random.randint(0,7)):\n",
    "    \"\"\"Function merged data frame - one data frame for randomly selected subject from control group and \n",
    "    one data frame for randomly selected subject from alcoholic group\"\"\"\n",
    "    ## random choose the name_id of subject from alcoholic/control group\n",
    "    alcoholic_id = EEG_data['name'][(EEG_data['subject identifier'] == 'a') & \n",
    "                                    (EEG_data['matching condition'] == stimulus)].unique()[random_id]\n",
    "    control_id = EEG_data['name'][(EEG_data['subject identifier'] == 'c') & \n",
    "                                  (EEG_data['matching condition'] == stimulus)].unique()[random_id]\n",
    "    \n",
    "    ## get min trial numbers for each group\n",
    "    alcoholic_trial_number = EEG_data['trial number'][(EEG_data['name'] == alcoholic_id) & (EEG_data['matching condition'] == stimulus)].min()\n",
    "    control_trial_number = EEG_data['trial number'][(EEG_data['name'] == control_id) & (EEG_data['matching condition'] == stimulus)].min()\n",
    "\n",
    "    ## filter the EEG DF\n",
    "    alcoholic_df = EEG_data[(EEG_data['name'] == alcoholic_id) & (EEG_data['trial number'] == alcoholic_trial_number)]\n",
    "    control_df = EEG_data[(EEG_data['name'] == control_id) & (EEG_data['trial number'] == control_trial_number)]\n",
    "    \n",
    "    return alcoholic_df.append(control_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "47e33a775d12c0086d2c93ba82f0c79d5aee5424"
   },
   "outputs": [],
   "source": [
    "sensor_positions = EEG_data[['sensor position', 'channel']].drop_duplicates().reset_index(drop=True).drop(['channel'], axis=1).reset_index(drop=False).rename(columns={'index':'channel'})['sensor position']\n",
    "channels = EEG_data[['sensor position', 'channel']].drop_duplicates().reset_index(drop=True).drop(['channel'], axis=1).reset_index(drop=False).rename(columns={'index':'channel'})['channel']\n",
    "\n",
    "def plot_3dSurface_and_heatmap(stimulus, group, df):\n",
    "    \n",
    "    if group == 'c':\n",
    "        group_name = 'Control'\n",
    "    else:\n",
    "        group_name = 'Alcoholic'\n",
    "        \n",
    "    temp_df = pd.pivot_table(df[['channel', 'sample num', 'sensor value']][(df['subject identifier'] == group) & (df['matching condition'] == stimulus)],\n",
    "                                          index='channel', columns='sample num', values='sensor value').values.tolist()\n",
    "    data = [go.Surface(z=temp_df, colorscale='Bluered')]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='<br>3d Surface and Heatmap of Sensor Values for ' + stimulus + ' Stimulus for ' + group_name + ' Group',\n",
    "        width=800,\n",
    "        height=900,\n",
    "        autosize=False,\n",
    "        margin=dict(t=0, b=0, l=0, r=0),\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title='Time (sample num)',\n",
    "                gridcolor='rgb(255, 255, 255)',\n",
    "    #             erolinecolor='rgb(255, 255, 255)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(230, 230,230)'\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='Channel',\n",
    "                tickvals=channels,\n",
    "                ticktext=sensor_positions,\n",
    "                gridcolor='rgb(255, 255, 255)',\n",
    "                zerolinecolor='rgb(255, 255, 255)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(230, 230, 230)'\n",
    "            ),\n",
    "            zaxis=dict(\n",
    "                title='Sensor Value',\n",
    "                gridcolor='rgb(255, 255, 255)',\n",
    "                zerolinecolor='rgb(255, 255, 255)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(230, 230,230)'\n",
    "            ),\n",
    "            aspectratio = dict(x=1, y=1, z=0.5),\n",
    "            aspectmode = 'manual'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    updatemenus=list([\n",
    "        dict(\n",
    "            buttons=list([   \n",
    "                dict(\n",
    "                    args=['type', 'surface'],\n",
    "                    label='3D Surface',\n",
    "                    method='restyle'\n",
    "                ),\n",
    "                dict(\n",
    "                    args=['type', 'heatmap'],\n",
    "                    label='Heatmap',\n",
    "                    method='restyle'\n",
    "                )             \n",
    "            ]),\n",
    "            direction = 'left',\n",
    "            pad = {'r': 10, 't': 10},\n",
    "            showactive = True,\n",
    "            type = 'buttons',\n",
    "            x = 0.1,\n",
    "            xanchor = 'left',\n",
    "            y = 1.1,\n",
    "            yanchor = 'top' \n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    annotations = list([\n",
    "        dict(text='Trace type:', x=0, y=1.085, yref='paper', align='left', showarrow=False)\n",
    "    ])\n",
    "    layout['updatemenus'] = updatemenus\n",
    "    layout['annotations'] = annotations\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ccac0657bdc36881221d6c659bfdc21c0e2051e"
   },
   "source": [
    "## <a id='s1'>3.1. Data Sample for \"S1 obj\" Stimulus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "14ec22f3ba53428a19102c79113fb2f8b2fc1004"
   },
   "outputs": [],
   "source": [
    "stimulus = 'S1 obj'\n",
    "S1_sample_df = sample_data(stimulus=stimulus, random_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d8cffee4d8269c9a685e496fa7acf1432f50896"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='a', df=S1_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e2146a859761026f1af2150a937a94d81c4fc58"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='c', df=S1_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40c6d3326f71de3a313188dfe23b73b5491dab2f"
   },
   "source": [
    "## <a id='s2m'>3.2. Data Sample for \"S2 match\" Stimulus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44772f2c6ebc0afd0994159beb14969584270768"
   },
   "outputs": [],
   "source": [
    "stimulus = 'S2 match'\n",
    "S2_m_sample_df = sample_data(stimulus=stimulus, random_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "469d74b8148107eac477a18365554a079d19bd1d"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='a', df=S2_m_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "674773db20bd01813fb92e144347684c03451487"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='c', df=S2_m_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fcdc0679034a27ba22cc990bd661519a9fcf669"
   },
   "source": [
    "Some big spike happend here for control subject. High response values (which respond to front right region of the head) could be caused by eye movement or blink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b585efd3dd5bf6db63bc05126318c6df25620477"
   },
   "source": [
    "## <a id='s2nm'>3.3. Data Sample for \"S2 nomatch\" Stimulus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "264512919dfda888fd1f3aa17306a2fe158cf056"
   },
   "outputs": [],
   "source": [
    "stimulus = 'S2 nomatch'\n",
    "S2_nm_sample_df = sample_data(stimulus=stimulus, random_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92a8f3862dea84b1e776b9ba510403e1560efdda"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='a', df=S2_nm_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bf2f558c2ec1a221e2aae88335d0c5d1636250f"
   },
   "outputs": [],
   "source": [
    "plot_3dSurface_and_heatmap(stimulus=stimulus, group='c', df=S2_nm_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4b3d74dc9a144652bf2b911f48d8b5b9c97541e"
   },
   "source": [
    "Let's take a look at EEG topography for the last example (*S2 nomatch stimulus*) to see how brain regions are differ. I have found a terrific work of **Matt Craddock** at his [website blog](https://www.mattcraddock.com/blog/2017/02/25/erp-visualization-creating-topographical-scalp-maps-part-1/) on how to create EEG topography using R. I created sample topography plots and converted them into .gif animations, so here are the final result. You can see my implementation of Matt's code at [GitHub](https://github.com/ruslan-kl/EEG-data-analysis/blob/master/plot_topography.R).\n",
    "\n",
    "| **Alcoholic** | **Control** |\n",
    "|--|--|\n",
    "| <img src=\"https://i.ibb.co/kBmwypK/s2-nm-a.gif\" alt=\"s2-nm-a\" border=\"0\"> | <img src=\"https://i.ibb.co/R9SQG0j/s2-nm-c.gif\" alt=\"s2-nm-c\" border=\"0\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6466b9aec502e8c7ecf51c04f0f88709e75cbdf1"
   },
   "source": [
    "After the quick glimpse it looks like on average response values are higher for alcoholic group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee0cf3e061a518f82c8121d1076001287a26a4f3"
   },
   "source": [
    "## <a id='corr_sample'>3.4. Correlations between the Regions</a>\n",
    "\n",
    "Next step will be to investigate the correlations between firing rates of each sensor position. If two brain regions have high correlation value that means that they trend to wire together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6438be0d9eb7af393b00dd39fd46f1856b25f577"
   },
   "outputs": [],
   "source": [
    "## create the list of possible channel pairs\n",
    "sample_corr_df = pd.pivot_table(S2_nm_sample_df[S2_nm_sample_df['subject identifier'] == 'a'], values='sensor value', index='sample num', columns='sensor position').corr()\n",
    "\n",
    "list_of_pairs = []\n",
    "j = 0\n",
    "for column in sample_corr_df.columns:\n",
    "    j += 1\n",
    "    for i in range(j, len(sample_corr_df)):\n",
    "        if column != sample_corr_df.index[i]:\n",
    "            temp_pair = [column + '-' + sample_corr_df.index[i]]\n",
    "            list_of_pairs.append(temp_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bc16f2c6a30cfd403ce32fa39196c02c848828ba"
   },
   "outputs": [],
   "source": [
    "def get_correlated_pairs_sample(threshold, correlation_df, group):\n",
    "    ## create dictionary wheke keys are the pairs and values are the amount of high correlation pair\n",
    "    corr_pairs_dict = {}\n",
    "    for i in range(len(list_of_pairs)):\n",
    "        temp_corr_pair = dict(zip(list_of_pairs[i], [0]))\n",
    "        corr_pairs_dict.update(temp_corr_pair)\n",
    "\n",
    "    j = 0\n",
    "    for column in correlation_df.columns:\n",
    "        j += 1\n",
    "        for i in range(j, len(correlation_df)):\n",
    "            if ((correlation_df[column][i] >= threshold) & (column != correlation_df.index[i])):\n",
    "                corr_pairs_dict[column + '-' + correlation_df.index[i]] += 1\n",
    "\n",
    "    corr_count = pd.DataFrame(corr_pairs_dict, index=['count']).T.reset_index(drop=False).rename(columns={'index': 'channel_pair'})\n",
    "    print('Channel pairs that have correlation value >= ' + str(threshold) + ' (' + group + ' group):')\n",
    "    print(corr_count['channel_pair'][corr_count['count'] > 0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "fe46b5561c0c1de88d69fc5c492f949bb400502f"
   },
   "outputs": [],
   "source": [
    "def plot_sensors_correlation(df, threshold_value):\n",
    "    \"\"\"Funtion plots the the correlation plots between sensor positions for each group\"\"\"\n",
    "    correlations_alcoholic = pd.pivot_table(df[df['subject identifier'] == 'a'], \n",
    "                                          values='sensor value', index='sample num', columns='sensor position').corr()\n",
    "\n",
    "    correlations_control = pd.pivot_table(df[df['subject identifier'] == 'c'], \n",
    "                                          values='sensor value', index='sample num', columns='sensor position').corr()\n",
    "\n",
    "    fig = plt.figure(figsize=(17,10))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title('Alcoholic group', fontsize=14)\n",
    "    mask = np.zeros_like(correlations_alcoholic, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(correlations_alcoholic, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title('Control group', fontsize=14)\n",
    "    mask = np.zeros_like(correlations_control, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(correlations_control, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.suptitle('Correlation between Sensor Positions for ' + df['matching condition'].unique()[0] + ' stimulus', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    get_correlated_pairs_sample(threshold=threshold_value, correlation_df=correlations_alcoholic, group='Alcoholic')\n",
    "    print('\\n')\n",
    "    get_correlated_pairs_sample(threshold=threshold_value, correlation_df=correlations_control, group='Control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed3bc9b6956a51b499b46700f5bf60362bb400d5"
   },
   "outputs": [],
   "source": [
    "plot_sensors_correlation(df=S1_sample_df, threshold_value=.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfaef6ddda18f2691874bfae446f2629465c6e55"
   },
   "outputs": [],
   "source": [
    "plot_sensors_correlation(df=S2_m_sample_df, threshold_value=.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d285f5e590764c86e0a55796afa9247ff6567410"
   },
   "outputs": [],
   "source": [
    "plot_sensors_correlation(df=S2_nm_sample_df, threshold_value=.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e41fce9ff6d06de7f38522706574420ca12b0fa4"
   },
   "source": [
    "**What can we learn from these correlated pairs?**\n",
    "\n",
    "First of all, there were no surprises - high correlation appears between regions that are close to each other. Maybe we will see some unsurprised paris when looking on the full data.\n",
    "\n",
    "Secondly, you can notice the interesting trend that some regions of the brain show different correlation values among subjects (like [O1-PZ] vs [F2-FT8]). When correlations are high for alcoholic subject they are low for control subject and vice versa. Let's look back on *S1 obj* stimuli as an example:\n",
    "<img src=\"https://i.ibb.co/h1ty8fq/Deepin-Screenshot-select-area-20181228000405.png\" alt=\"Deepin-Screenshot-select-area-20181228000405\" border=\"0\">\n",
    "**P** stands for Parietal, **O** - Occipital (which both are closer to the back of the head), **F** - Frontal, **C** - Central parts of the brain. \n",
    "\n",
    "For alcoholic subject the higher the values in **P** and **O** regions, the lower are the values in **F** and **C** regions (and vice versa). But for the control subject the correlation for this regions is positive (the higher the one, the higher the other). And for each stimulus the picture is all the way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2390412fe06a538f15c1af4a08c63ed48da343ea"
   },
   "source": [
    "# <a id='whole_set'>4. Whole EEG Data Set Analysis</a>\n",
    "\n",
    "For now we've been looking on example of one subject from each group. Now we will see how does the picture changes when we apply the same analysis of the whole data set. Let's start on finding the correlation pairs for each trial.\n",
    "\n",
    "## <a id='corr'>4.1. Correlations between the Regions</a>\n",
    "\n",
    "What exactly I am going to do is to find top 20 correlated pairs for each stimuli among two groups and plot them together to see if these pairs are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3cd8cea03b643622942959f5c6c6f77281484444"
   },
   "outputs": [],
   "source": [
    "def get_correlated_pairs(stimulus, threshold, group):\n",
    "    \"\"\"Funtion returns the df which holds pairs of channel with high correlation for stimulus, group and threshold provided\"\"\"\n",
    "    corr_pairs_dict = {}\n",
    "    trial_numbers_list = EEG_data['trial number'][(EEG_data['subject identifier'] == group) & (EEG_data['matching condition'] == stimulus)].unique()\n",
    "    ## create dictionary wheke keys are the pairs and values are the amount of high correlation pair\n",
    "    for i in range(len(list_of_pairs)):\n",
    "        temp_corr_pair = dict(zip(list_of_pairs[i], [0]))\n",
    "        corr_pairs_dict.update(temp_corr_pair)\n",
    "\n",
    "    for trial_number in trial_numbers_list:    \n",
    "        correlation_df = pd.pivot_table(EEG_data[(EEG_data['subject identifier'] == group) & (EEG_data['trial number'] == trial_number)], \n",
    "                                        values='sensor value', index='sample num', columns='sensor position').corr()\n",
    "\n",
    "        j = 0 ## by setting the j we are going just through values below the main diagonal\n",
    "        for column in correlation_df.columns:\n",
    "            j += 1\n",
    "            for i in range(j, len(correlation_df)):\n",
    "                if ((correlation_df[column][i] >= threshold) & (column != correlation_df.index[i])):\n",
    "                    corr_pairs_dict[column + '-' + correlation_df.index[i]] += 1\n",
    "\n",
    "    corr_count = pd.DataFrame(corr_pairs_dict, index=['count']).T.reset_index(drop=False).rename(columns={'index': 'channel_pair'})\n",
    "    corr_count['group'] = group\n",
    "    corr_count['stimulus'] = stimulus\n",
    "    return(corr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "656a7fbaaa8f9a42f17782dce0c5acba38384d75"
   },
   "outputs": [],
   "source": [
    "def compare_corr_pairs(stimulus):\n",
    "    \"\"\"Function creates bar chart with the ratio of correlated pairs for both groups\"\"\"\n",
    "    top_control_df = corr_pairs_df[(corr_pairs_df['group'] == 'c') & (corr_pairs_df['stimulus'] == stimulus)]\n",
    "    top_alcoholic_df = corr_pairs_df[(corr_pairs_df['group'] == 'a') & (corr_pairs_df['stimulus'] == stimulus)]\n",
    "    top_control_pairs = top_control_df.sort_values('count', ascending=False)['channel_pair'][:20]\n",
    "    top_alcoholic_pairs = top_alcoholic_df.sort_values('count', ascending=False)['channel_pair'][:20]\n",
    "\n",
    "    merged_df = pd.DataFrame({'channel_pair': top_control_pairs.append(top_alcoholic_pairs).unique()})\n",
    "    merged_df = merged_df.merge(top_control_df[['channel_pair', 'count', 'trials_count']],\n",
    "                                on='channel_pair', how='left').rename(columns={'count':'count_control', 'trials_count': 'trials_count_c'})\n",
    "    merged_df = merged_df.merge(top_alcoholic_df[['channel_pair', 'count', 'trials_count']],\n",
    "                                on='channel_pair', how='left').rename(columns={'count':'count_alcoholic', 'trials_count': 'trials_count_a'})\n",
    "\n",
    "    data_1 = go.Bar(x=merged_df['channel_pair'],\n",
    "                    y=(merged_df['count_alcoholic']/merged_df['trials_count_a']).apply(lambda x: round(x,2)),\n",
    "                    text=merged_df['count_alcoholic'],\n",
    "                    name='Alcoholic Group',\n",
    "                    marker=dict(color='rgb(20,140,45)'))\n",
    "\n",
    "    data_2 = go.Bar(x=merged_df['channel_pair'],\n",
    "                    y=(merged_df['count_control']/merged_df['trials_count_c']).apply(lambda x: round(x,2)),\n",
    "                    text=merged_df['count_control'],\n",
    "                    name='Control Group',\n",
    "                    marker=dict(color='rgb(200,100,45)'))\n",
    "\n",
    "    layout = go.Layout(title='Amount of Correlated Pairs for the whole Data Set (' + stimulus + ' stimulus)',\n",
    "                       xaxis=dict(title='Channel Pairs'),\n",
    "                       yaxis=dict(title='Ratio'),\n",
    "                       barmode='group')\n",
    "\n",
    "    data = [data_1, data_2]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c08a291f6be110389f3f11ba4769544c2dd77a9"
   },
   "outputs": [],
   "source": [
    "corr_pairs_df = pd.DataFrame({})\n",
    "stimuli_list = ['S1 obj', 'S2 match', 'S2 nomatch']\n",
    "## create df that holds information of total trial amount for each subject by stimulus\n",
    "size_df = EEG_data.groupby(['subject identifier', 'matching condition'])[['trial number']].nunique().reset_index(drop=False).rename(columns={'trial number':'trials_count'})\n",
    "\n",
    "for stimulus in stimuli_list:\n",
    "    corr_pairs_df = corr_pairs_df.append(get_correlated_pairs(stimulus=stimulus, threshold=.9, group='c'))\n",
    "    corr_pairs_df = corr_pairs_df.append(get_correlated_pairs(stimulus=stimulus, threshold=.9, group='a'))\n",
    "corr_pairs_df = corr_pairs_df.merge(size_df, left_on=['group', 'stimulus'], right_on=['subject identifier', 'matching condition'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8eaef5499d74792659ecaaf095017bd45ded2dad"
   },
   "outputs": [],
   "source": [
    "compare_corr_pairs(stimulus='S1 obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f8f65d43ae55d2ecea0711c14c2bf6d166cd46e"
   },
   "outputs": [],
   "source": [
    "compare_corr_pairs(stimulus='S2 match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0768e8221e422fd21e7f71a8b0b1843179c07ed0"
   },
   "outputs": [],
   "source": [
    "compare_corr_pairs(stimulus='S2 nomatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee9cd32cd085365127aa985e4a36d7f7d00a6a2f"
   },
   "source": [
    "**What can we see here?**\n",
    "\n",
    "Again, no surprises, all high correlations are seems to be between regions that are close to each other. \n",
    "\n",
    "Some interesting insight: **FP1-FPZ** and **FP1-FP2** pairs showed the high correlation value for control subjects >90% of the time. But for the alcoholic group this ration is much lower (~20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11b4a1509c80b0e586e0319a728d387ba10a8875"
   },
   "source": [
    "## <a id='avg_comp'>4.2. Compare the Average Response Values</a>\n",
    "\n",
    "Now I am going to check whether the response values among two groups show significantly different result. To do so I will run [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test).\n",
    "\n",
    "* **Null Hypothesis**: No difference for response values for the single channel between Alcoholic and Controls Groups\n",
    "* **Alternative Hypothesis**: There is a significant difference for response values between Alcoholic and Controls Groups (*two-tailed test*)\n",
    "* **Significance level $\\alpha$** = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79f8ba1931d861ad08067d6ce1596f91878c1aa3"
   },
   "outputs": [],
   "source": [
    "stimulus_list = EEG_data['matching condition'].unique().tolist() ## list of stimuli\n",
    "channels_list = EEG_data['channel'].unique().tolist() ## list of channels\n",
    "\n",
    "## get the Average Sensor Values for each channel by Subject group and Stimulus\n",
    "agg_df = EEG_data.groupby(['subject identifier', 'matching condition', 'sensor position'], as_index=False)[['sensor value']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c26f686043ce9dbd7ae9f1efacdfdfd3e0fe21a1"
   },
   "outputs": [],
   "source": [
    "def get_p_value(stimulus, sensor):\n",
    "    \"\"\"\n",
    "    Funtion takes the stimulus parameter and channel number and returns the p-value from Mann Whitney U-test (Alcoholic vs Control).\n",
    "    \"\"\"\n",
    "    x = EEG_data[['sensor value']][(EEG_data['subject identifier'] == 'a') & \n",
    "                                   (EEG_data['matching condition'] == stimulus) & \n",
    "                                   (EEG_data['sensor position'] == sensor)]\n",
    "    y = EEG_data[['sensor value']][(EEG_data['subject identifier'] == 'c') & \n",
    "                                   (EEG_data['matching condition'] == stimulus) & \n",
    "                                   (EEG_data['sensor position'] == sensor)]\n",
    "    stat, p = mannwhitneyu(x=x, \n",
    "                           y=y,\n",
    "                           alternative='two-sided')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc23bc86b931f8f15ca7a4b78b9fc74938d5b74b"
   },
   "outputs": [],
   "source": [
    "## create empty df that will hold info about the statistica test\n",
    "stat_test_results = pd.DataFrame({'stimulus': [], \n",
    "                                  'sensor': [],\n",
    "                                  'p_value': []})\n",
    "\n",
    "for sensor in tqdm(EEG_data['sensor position'].unique()):\n",
    "    for stimulus in EEG_data['matching condition'].unique():\n",
    "        temp_df = pd.DataFrame({'stimulus': stimulus,\n",
    "                                'sensor': sensor,\n",
    "                                'p_value': get_p_value(stimulus=stimulus, sensor=sensor)},\n",
    "                               index=[0])\n",
    "        stat_test_results = stat_test_results.append(temp_df)\n",
    "        \n",
    "stat_test_results = stat_test_results.reset_index(drop=True)\n",
    "stat_test_results['reject_null'] = stat_test_results['p_value'] <= 0.05 ## check whether we can reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38ca102fecbd517f92020b375d93f08bae5b38a0"
   },
   "source": [
    "Let's look at the ratio of how many responses were significantly different among two groups across all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "166bd5a901687fee98b2cc41fb266cae70ccfd12"
   },
   "outputs": [],
   "source": [
    "stat_test_results.groupby(['stimulus'])[['reject_null']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc8c378c591a8048ed56c168090ce36cee9a963f"
   },
   "source": [
    "Doesn't it looks impressive? \n",
    "* ~90% of channels (~55 channels) responed significanlty different when single object was shown;\n",
    "* ~77% of channels (~47 channels) responed significanlty different when two identical objects were shown;\n",
    "* ~80% of channels (~50 channels) responed significanlty different when two non-identical objects were shown.\n",
    "\n",
    "We can take a look at each channel to see which stimuli showed the significant difference among two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "26fd88f53e9695ff6f71442f6bf31e013b2cccf8"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for stimulus in stimulus_list:\n",
    "    trace = go.Bar(x=stat_test_results['sensor'][stat_test_results['stimulus'] == stimulus],\n",
    "                    y=stat_test_results['reject_null'][stat_test_results['stimulus'] == stimulus],\n",
    "                    name=stimulus)\n",
    "    data.append(trace)\n",
    "\n",
    "layout = go.Layout(title='Amount of Significant Differences for each Channel',\n",
    "                   xaxis=dict(title='Sensor Position'),\n",
    "                   yaxis=dict(title='Is Significant',\n",
    "                              showticklabels=False),\n",
    "                   barmode='stack')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "128ce2beee8e17d56f9db6c8f40c077435706edf"
   },
   "outputs": [],
   "source": [
    "# ## df that will be needed to plot the 'sesnors' that showed the difference for 3 stimuli in different color on ML future importance plot\n",
    "# color_scale_df = stat_test_results.groupby(['sensor'])[['reject_null']].sum()\n",
    "# color_scale_df['color'] = ''\n",
    "# color_scale_df.loc[color_scale_df['reject_null'] != 3, 'color'] = 'rgba(48,203,231,0.7)'\n",
    "# color_scale_df.loc[color_scale_df['reject_null'] == 3, 'color'] = 'rgba(222,45,38,0.8)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad5a08b42f92de54cadcc093eacd16a9fdd5e207"
   },
   "source": [
    " Now we can take a look on those region where the difference is significant for each stimulus:\n",
    "<img src=\"https://i.ibb.co/cJfK74n/Deepin-Screenshot-select-area-20190105172357.png\" alt=\"Deepin-Screenshot-select-area-20190105172357\" border=\"0\">\n",
    "Well, it is hard for me to say that there is one specific region of the brain where the response values are significantly different among two groups - it is more like these region are spreaded all around. However, it's still interesting to see what are those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3eb407219b5e6c28da84b77a9ba747527328fafb"
   },
   "source": [
    "# <a id='conclusion'>5. Conclusions</a> \n",
    "\n",
    "It was nice experience for me on how work and visualize EEG data. \n",
    "\n",
    "Correlation analysis showed that **FP1-FPZ** and **FP1-FP2** pairs showed the high correlation value for control subjects ~90% of the time. But for the alcoholic group this ration is much lower ~20%.\n",
    "\n",
    "Statistical analysis showed that **34 regions** (of 61) responded significanlty different for each of the stimuli among two groups (with significance level = .05).\n",
    "\n",
    "Another interesting approach would be creating ML model to predict the class of the subject (**A** or **C**) based on response values. It might be possible to do with full dataset from UC Irvine Machine Learning Repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
